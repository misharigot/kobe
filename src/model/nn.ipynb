{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/misharigot/kobe/blob/master/src/model/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pl5KceKr8HWB"
   },
   "source": [
    "This notebook contains the neural network to predict kobe's shots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQF73eWo8HWF"
   },
   "source": [
    "## To Do from Trello\n",
    "- [x] Implementeren van cross validation.\n",
    "- [ ] Connecten van nieuwe cross validation module met de nn model module.\n",
    "- [ ] Bouwen van verschillende netwerken (vorm, aantal nodes etc.)\n",
    "- [ ] Kijken welke loss function we moeten gebruiken, cross entropy vs log loss. Log loss sowieso proberen om te vergelijken met competition entries.\n",
    "- [ ] Implementeren van model export functie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P32ha3sG8HWH",
    "outputId": "57c6a5d5-0c5d-4301-d86c-be20ad40023d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.insert(0, '..')  # Needed to make the import below work\n",
    "\n",
    "from multiple_train_test_splits import MultipleTrainTestSplits\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Combine the minutes and seconds remaining columns into one column.\n",
    "    \"\"\"\n",
    "    df['minutes_remaining'] = df['minutes_remaining'].astype(int)\n",
    "    df['seconds_remaining'] = df['seconds_remaining'].astype(int)\n",
    "\n",
    "    # Combine minutes and seconds remaining into decimal minutes remaining, e.g. 6.5 for 6 mins and 30 secs.\n",
    "    df['time_remaining'] = round(df['minutes_remaining'] + (df['seconds_remaining'] / 60), 2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"One-hot encode all categorical columns.\n",
    "    \"\"\"\n",
    "     # Categorize all columns based on their data type\n",
    "    categorical_columns = [\n",
    "        'action_type',\n",
    "        'combined_shot_type',\n",
    "        'game_event_id', # Meaning?\n",
    "        'game_id',\n",
    "        'season',\n",
    "        'shot_type',\n",
    "        'shot_zone_area',\n",
    "        'shot_zone_basic',\n",
    "        'shot_zone_range',\n",
    "        'team_id',\n",
    "        'team_name',\n",
    "        'matchup',\n",
    "        'opponent'\n",
    "    ]\n",
    "\n",
    "    temporal_columns = [\n",
    "        'game_date'\n",
    "    ]\n",
    "\n",
    "    remaining_columns = [\n",
    "        'lat',\n",
    "        'loc_x',\n",
    "        'loc_y',\n",
    "        'lon',\n",
    "        'period',\n",
    "        'shot_distance',\n",
    "        'time_remaining',\n",
    "        'shot_made_flag'  # y label\n",
    "    ]\n",
    "\n",
    "    excluded_columns = [\n",
    "        'shot_id',            # Just an auto-increment id, does not mean anything\n",
    "        'minutes_remaining',  # Not needed, since we use the engineered field 'time_remaining'\n",
    "        'seconds_remaining'   # Not needed, since we use the engineered field 'time_remaining'\n",
    "    ]\n",
    "\n",
    "    # Convert relevant columns to categorical columns\n",
    "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
    "    df_with_only_categoricals = df[categorical_columns]\n",
    "    \n",
    "    print(df_with_only_categoricals.describe())\n",
    "    print('#######################################################\\n####')\n",
    "    \n",
    "    # One hot encode categorical columns\n",
    "    encoder = preprocessing.OneHotEncoder()\n",
    "    encoder.fit(df_with_only_categoricals)\n",
    "    one_hot_encoded_df = pd.DataFrame(encoder.transform(df_with_only_categoricals).toarray())\n",
    "    \n",
    "\n",
    "    # Combine the one hot encoded part of the df with the remaining df\n",
    "    non_categorical_df = df[remaining_columns]\n",
    "    resulting_df = pd.concat([one_hot_encoded_df, non_categorical_df], axis=1)\n",
    "    return resulting_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Returns the features.\n",
    "    \"\"\"\n",
    "    temp = data.copy()\n",
    "    X = temp.drop(columns=['shot_made_flag'])\n",
    "    return X\n",
    "\n",
    "def get_y(data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Returns the target.\n",
    "    \"\"\"\n",
    "    return data['shot_made_flag'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data: pd.DataFrame) -> np.array:\n",
    "    \"\"\"Preprocess the raw kobe data from Kaggle.\n",
    "    \"\"\"\n",
    "    df = combine_time(data)\n",
    "    df = one_hot_encode(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim: int):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sbi8PrU8HXm",
    "outputId": "79b3a233-10c7-4e46-fe67-a71c3d7917b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       action_type combined_shot_type  game_event_id   game_id   season  \\\n",
      "count         5141               5141           5141      5141     5141   \n",
      "unique          26                  5            557       291        4   \n",
      "top      Jump Shot          Jump Shot            313  20200069  2002-03   \n",
      "freq          3235               3817             22        39     1598   \n",
      "\n",
      "             shot_type shot_zone_area shot_zone_basic  shot_zone_range  \\\n",
      "count             5141           5141            5141             5141   \n",
      "unique               2              6               7                5   \n",
      "top     2PT Field Goal      Center(C)       Mid-Range  Less Than 8 ft.   \n",
      "freq              4422           2408            2300             1742   \n",
      "\n",
      "           team_id           team_name      matchup opponent  \n",
      "count         5141                5141         5141     5141  \n",
      "unique           1                   1           60       30  \n",
      "top     1610612747  Los Angeles Lakers  LAL vs. SAS      SAS  \n",
      "freq          5141                5141          180      320  \n",
      "#######################################################\n",
      "####\n",
      "       action_type combined_shot_type  game_event_id   game_id   season  \\\n",
      "count         5139               5139           5139      5139     5139   \n",
      "unique          42                  6            557       267        5   \n",
      "top      Jump Shot          Jump Shot            398  20601081  2005-06   \n",
      "freq          3346               3942             24        41     1792   \n",
      "\n",
      "             shot_type shot_zone_area shot_zone_basic shot_zone_range  \\\n",
      "count             5139           5139            5139            5139   \n",
      "unique               2              6               7               5   \n",
      "top     2PT Field Goal      Center(C)       Mid-Range       16-24 ft.   \n",
      "freq              3856           2036            2101            1524   \n",
      "\n",
      "           team_id           team_name      matchup opponent  \n",
      "count         5139                5139         5139     5139  \n",
      "unique           1                   1           64       30  \n",
      "top     1610612747  Los Angeles Lakers  LAL vs. HOU      PHX  \n",
      "freq          5139                5139          172      282  \n",
      "#######################################################\n",
      "####\n",
      "Epoch 1/2\n",
      "5141/5141 [==============================] - 2s 396us/step - loss: 0.7504 - accuracy: 0.5660\n",
      "Epoch 2/2\n",
      "5141/5141 [==============================] - 2s 363us/step - loss: 0.6866 - accuracy: 0.6042\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_75_input to have shape (1002,) but got array with shape (1000,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-9dc356bfbf9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mloss_and_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/vu/machine-learning/kobe/.venv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vu/machine-learning/kobe/.venv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vu/machine-learning/kobe/.venv/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_75_input to have shape (1002,) but got array with shape (1000,)"
     ]
    }
   ],
   "source": [
    "mtts = MultipleTrainTestSplits(csv_path='../../data/data.csv')\n",
    "\n",
    "test_set = mtts.test_set\n",
    "\n",
    "loss_and_metrics = []\n",
    "\n",
    "for train_set, validation_set in mtts.train_validation_split(as_dataframe=True):\n",
    "    # Preprocess the training set\n",
    "    preprocessed_train_set = preprocess(train_set)\n",
    "    # Split the features from the target\n",
    "    x_train = get_x(preprocessed_train_set)\n",
    "    y_train = get_y(preprocessed_train_set)\n",
    "\n",
    "    # Preprocess the validation set\n",
    "    preprocessed_validation_set = preprocess(validation_set)\n",
    "    # Split the features from the target\n",
    "    x_validation = get_x(preprocessed_validation_set)\n",
    "    y_validation = get_y(preprocessed_validation_set)\n",
    "\n",
    "    input_dim = x_train.shape[1]  # number of columns (dimensions for the input layer of the model)\n",
    "    \n",
    "    model = create_model(input_dim=input_dim)\n",
    "    model.fit(x_train, y_train, epochs=2, batch_size=32)\n",
    "\n",
    "    loss_and_metrics.append(model.evaluate(x_validation, y_validation, batch_size=128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXdEfk8U8HXz",
    "outputId": "93c02ec8-0302-466c-98da-328924921432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52622545],\n",
       "       [0.26044014],\n",
       "       [0.9890055 ],\n",
       "       ...,\n",
       "       [0.6824401 ],\n",
       "       [0.39076325],\n",
       "       [0.05106422]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = model.predict(x_validation, batch_size=128)\n",
    "classes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kobe",
   "language": "python",
   "name": "kobe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
