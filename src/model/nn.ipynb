{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "kobe",
      "language": "python",
      "name": "kobe"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/misharigot/kobe/blob/master/src/model/nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pl5KceKr8HWB"
      },
      "source": [
        "This notebook contains the neural network to predict kobe's shots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PQF73eWo8HWF"
      },
      "source": [
        "## To Do from Trello\n",
        "- [x] Implementeren van cross validation.\n",
        "- [ ] Connecten van nieuwe cross validation module met de nn model module.\n",
        "- [ ] Bouwen van verschillende netwerken (vorm, aantal nodes etc.)\n",
        "- [ ] Kijken welke loss function we moeten gebruiken, cross entropy vs log loss. Log loss sowieso proberen om te vergelijken met competition entries.\n",
        "- [ ] Implementeren van model export functie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UNP7SAMudAib",
        "outputId": "b9f946c7-8030-4d64-8c66-c5c8de52f91a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# When using this notebook in Google Colab, clone the repo in the file system in\n",
        "# order to use the python modules from the repo.\n",
        "!git  clone https://github.com/misharigot/kobe.git\n",
        "!ls -lsa"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'kobe' already exists and is not an empty directory.\n",
            "total 20\n",
            "4 drwxr-xr-x 1 root root 4096 Mar 29 09:40 .\n",
            "4 drwxr-xr-x 1 root root 4096 Mar 29 09:39 ..\n",
            "4 drwxr-xr-x 1 root root 4096 Mar 25 16:11 .config\n",
            "4 drwxr-xr-x 6 root root 4096 Mar 29 09:40 kobe\n",
            "4 drwxr-xr-x 1 root root 4096 Mar 18 16:23 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E8ig3DuquVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys; sys.path.insert(0, '..')  # Needed to make the import below work\n",
        "\n",
        "# Use the line below in Colab\n",
        "from kobe.src.multiple_train_test_splits import MultipleTrainTestSplits\n",
        "from kobe.src.preprocessor import Preprocessor\n",
        "\n",
        "# Use the line below in a local env\n",
        "# from multiple_train_test_splits import MultipleTrainTestSplits\n",
        "# from preprocessor import Preprocessor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhD46PMLraSH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "8a4369c7-107c-4212-ccd3-fb6c0b4f03c0"
      },
      "source": [
        "# Because colab uses a different Keras version than we do.\n",
        "!pip install keras==\"2.3.1\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P32ha3sG8HWH",
        "outputId": "80e06d1a-9c0d-49b1-d4e1-d8454d93364d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JS4xXHyQGCCv",
        "colab": {}
      },
      "source": [
        "def get_x(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Returns the features.\n",
        "    \"\"\"\n",
        "    X = data.drop(columns=['shot_made_flag'])\n",
        "    return X\n",
        "\n",
        "def get_y(data: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Returns the target.\n",
        "    \"\"\"\n",
        "    Y = data['shot_made_flag'].copy()\n",
        "    return Y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g0Rei6a8GCC1",
        "colab": {}
      },
      "source": [
        "def create_model_1(input_dim: int):\n",
        "    \"\"\"Simple one hidden layer network.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=32, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='rmsprop',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_model_2(input_dim: int):\n",
        "    \"\"\"2 hidden layers network.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(units=64, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=32, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='rmsprop',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_model_3(input_dim: int):\n",
        "    \"\"\"1 hidden layer network with a lot of neurons.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(units=int(input_dim/2), activation='relu', input_dim=input_dim))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='rmsprop',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_model_4(input_dim: int):\n",
        "    \"\"\"2 hidden layers network with more neurons per layer.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(units=int(input_dim/2), activation='relu', input_dim=input_dim))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=int(input_dim/4), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='rmsprop',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_model_5(input_dim: int):\n",
        "    \"\"\"3 hidden layers network.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(units=128, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=32, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='rmsprop',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_model_6(input_dim: int):\n",
        "    \"\"\"Simple one hidden layer network with double the dim of model 1.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=64, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='rmsprop',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_models_dict():\n",
        "    models = {}\n",
        "    # models['model_1'] = create_model_1\n",
        "    # models['model_2'] = create_model_2\n",
        "    # models['model_3'] = create_model_3\n",
        "    # models['model_4'] = create_model_4\n",
        "    # models['model_5'] = create_model_5\n",
        "    models['model_6'] = create_model_6\n",
        "\n",
        "    return models\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lpR1vsRqcVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use in Colab\n",
        "csv_path = 'kobe/data/data.csv'\n",
        "\n",
        "# Use in local env\n",
        "# csv_path = '../../data/data.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5sbi8PrU8HXm",
        "outputId": "33ac11d7-d76a-4991-d993-63e703c2f312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mtts = MultipleTrainTestSplits(csv_path=csv_path)\n",
        "pp = Preprocessor(path_to_raw_data=csv_path)\n",
        "\n",
        "test_set = mtts.test_set\n",
        "\n",
        "loss_and_metrics = {}\n",
        "models = get_models_dict()\n",
        "\n",
        "# Loop over the models\n",
        "for model_name, model_func in models.items():\n",
        "    # checkpoint_path = \"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "    \n",
        "    loss_and_metrics[model_name] = {}\n",
        "    \n",
        "    # Loop over the train/validation splits/folds\n",
        "    n_fold = 0\n",
        "    for train_set, validation_set in mtts.train_validation_split(as_dataframe=True):\n",
        "        n_fold += 1\n",
        "        print(f'Training model: {model_name}, Fold: {n_fold}')\n",
        "        checkpoint_path = f\"{model_name}_fold_{n_fold}_weights-improvement\" + \"-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "\n",
        "        # Preprocess the training set\n",
        "        preprocessed_train_set = pp.preprocess(train_set)\n",
        "        # Split the features from the target\n",
        "        x_train = get_x(preprocessed_train_set)\n",
        "        y_train = get_y(preprocessed_train_set)\n",
        "\n",
        "        # Preprocess the validation set\n",
        "        preprocessed_validation_set= pp.preprocess(validation_set)\n",
        "        # Split the features from the target\n",
        "        x_validation = get_x(preprocessed_validation_set)\n",
        "        y_validation = get_y(preprocessed_validation_set)\n",
        "\n",
        "        input_dim = x_train.shape[1]  # number of columns (dimensions for the input layer of the model)\n",
        "        \n",
        "        # model = create_model(input_dim=input_dim)\n",
        "        model = model_func(input_dim)\n",
        "\n",
        "#         # Create model checkpoint to be able to resume at a checkpoint when training crashes.\n",
        "#         checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "#         callbacks_list = [checkpoint]\n",
        "\n",
        "        # Fit the model\n",
        "#         model.fit(x_train, y_train, epochs=2, batch_size=10, \n",
        "#                   validation_data=(x_validation, y_validation),\n",
        "#                   callbacks=callbacks_list, verbose=0)\n",
        "        model.fit(x_train, y_train, epochs=50, batch_size=128)\n",
        "    \n",
        "        loss_and_metrics[model_name][n_fold] = model.evaluate(x_validation, y_validation, batch_size=128)\n",
        "    "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model: model_6, Fold: 1\n",
            "Epoch 1/50\n",
            "5141/5141 [==============================] - 2s 325us/step - loss: 0.6823 - accuracy: 0.5960\n",
            "Epoch 2/50\n",
            "5141/5141 [==============================] - 2s 300us/step - loss: 0.6504 - accuracy: 0.6265\n",
            "Epoch 3/50\n",
            "5141/5141 [==============================] - 2s 298us/step - loss: 0.6346 - accuracy: 0.6563\n",
            "Epoch 4/50\n",
            "5141/5141 [==============================] - 2s 305us/step - loss: 0.6221 - accuracy: 0.6707\n",
            "Epoch 5/50\n",
            "5141/5141 [==============================] - 2s 303us/step - loss: 0.6110 - accuracy: 0.6746\n",
            "Epoch 6/50\n",
            "5141/5141 [==============================] - 2s 303us/step - loss: 0.6038 - accuracy: 0.6787\n",
            "Epoch 7/50\n",
            "5141/5141 [==============================] - 2s 306us/step - loss: 0.6024 - accuracy: 0.6839\n",
            "Epoch 8/50\n",
            "5141/5141 [==============================] - 2s 302us/step - loss: 0.5949 - accuracy: 0.6822\n",
            "Epoch 9/50\n",
            "5141/5141 [==============================] - 2s 301us/step - loss: 0.5938 - accuracy: 0.6824\n",
            "Epoch 10/50\n",
            "5141/5141 [==============================] - 2s 300us/step - loss: 0.5893 - accuracy: 0.6810\n",
            "Epoch 11/50\n",
            "5141/5141 [==============================] - 2s 297us/step - loss: 0.5893 - accuracy: 0.6855\n",
            "Epoch 12/50\n",
            "5141/5141 [==============================] - 2s 295us/step - loss: 0.5810 - accuracy: 0.6905\n",
            "Epoch 13/50\n",
            "5141/5141 [==============================] - 2s 296us/step - loss: 0.5818 - accuracy: 0.6897\n",
            "Epoch 14/50\n",
            "5141/5141 [==============================] - 2s 304us/step - loss: 0.5797 - accuracy: 0.6899\n",
            "Epoch 15/50\n",
            "5141/5141 [==============================] - 2s 298us/step - loss: 0.5762 - accuracy: 0.6901\n",
            "Epoch 16/50\n",
            "5141/5141 [==============================] - 2s 301us/step - loss: 0.5741 - accuracy: 0.6973\n",
            "Epoch 17/50\n",
            "5141/5141 [==============================] - 2s 297us/step - loss: 0.5707 - accuracy: 0.6981\n",
            "Epoch 18/50\n",
            "5141/5141 [==============================] - 2s 303us/step - loss: 0.5662 - accuracy: 0.6987\n",
            "Epoch 19/50\n",
            "5141/5141 [==============================] - 2s 296us/step - loss: 0.5680 - accuracy: 0.6983\n",
            "Epoch 20/50\n",
            "5141/5141 [==============================] - 2s 302us/step - loss: 0.5607 - accuracy: 0.7024\n",
            "Epoch 21/50\n",
            "5141/5141 [==============================] - 2s 296us/step - loss: 0.5605 - accuracy: 0.7032\n",
            "Epoch 22/50\n",
            "5141/5141 [==============================] - 2s 300us/step - loss: 0.5613 - accuracy: 0.7018\n",
            "Epoch 23/50\n",
            "5141/5141 [==============================] - 2s 299us/step - loss: 0.5564 - accuracy: 0.7080\n",
            "Epoch 24/50\n",
            "5141/5141 [==============================] - 2s 299us/step - loss: 0.5552 - accuracy: 0.7117\n",
            "Epoch 25/50\n",
            "5141/5141 [==============================] - 2s 298us/step - loss: 0.5570 - accuracy: 0.7059\n",
            "Epoch 26/50\n",
            "5141/5141 [==============================] - 2s 300us/step - loss: 0.5529 - accuracy: 0.7102\n",
            "Epoch 27/50\n",
            "5141/5141 [==============================] - 2s 303us/step - loss: 0.5469 - accuracy: 0.7174\n",
            "Epoch 28/50\n",
            "5141/5141 [==============================] - 2s 303us/step - loss: 0.5470 - accuracy: 0.7180\n",
            "Epoch 29/50\n",
            "5141/5141 [==============================] - 2s 304us/step - loss: 0.5471 - accuracy: 0.7168\n",
            "Epoch 30/50\n",
            "5141/5141 [==============================] - 2s 297us/step - loss: 0.5454 - accuracy: 0.7220\n",
            "Epoch 31/50\n",
            "5141/5141 [==============================] - 2s 296us/step - loss: 0.5414 - accuracy: 0.7178\n",
            "Epoch 32/50\n",
            "5141/5141 [==============================] - 2s 300us/step - loss: 0.5433 - accuracy: 0.7172\n",
            "Epoch 33/50\n",
            "5141/5141 [==============================] - 2s 304us/step - loss: 0.5364 - accuracy: 0.7213\n",
            "Epoch 34/50\n",
            "5141/5141 [==============================] - 2s 303us/step - loss: 0.5360 - accuracy: 0.7271\n",
            "Epoch 35/50\n",
            "5141/5141 [==============================] - 2s 302us/step - loss: 0.5366 - accuracy: 0.7255\n",
            "Epoch 36/50\n",
            "5141/5141 [==============================] - 2s 297us/step - loss: 0.5339 - accuracy: 0.7283\n",
            "Epoch 37/50\n",
            "5141/5141 [==============================] - 2s 299us/step - loss: 0.5332 - accuracy: 0.7253\n",
            "Epoch 38/50\n",
            "5141/5141 [==============================] - 2s 302us/step - loss: 0.5312 - accuracy: 0.7302\n",
            "Epoch 39/50\n",
            "5141/5141 [==============================] - 2s 302us/step - loss: 0.5291 - accuracy: 0.7300\n",
            "Epoch 40/50\n",
            "5141/5141 [==============================] - 2s 299us/step - loss: 0.5271 - accuracy: 0.7322\n",
            "Epoch 41/50\n",
            "5141/5141 [==============================] - 2s 298us/step - loss: 0.5264 - accuracy: 0.7337\n",
            "Epoch 42/50\n",
            "5141/5141 [==============================] - 2s 304us/step - loss: 0.5199 - accuracy: 0.7374\n",
            "Epoch 43/50\n",
            "5141/5141 [==============================] - 2s 295us/step - loss: 0.5249 - accuracy: 0.7312\n",
            "Epoch 44/50\n",
            "5141/5141 [==============================] - 2s 294us/step - loss: 0.5213 - accuracy: 0.7401\n",
            "Epoch 45/50\n",
            "5141/5141 [==============================] - 1s 288us/step - loss: 0.5178 - accuracy: 0.7415\n",
            "Epoch 46/50\n",
            "5141/5141 [==============================] - 2s 296us/step - loss: 0.5160 - accuracy: 0.7415\n",
            "Epoch 47/50\n",
            "5141/5141 [==============================] - 2s 296us/step - loss: 0.5137 - accuracy: 0.7423\n",
            "Epoch 48/50\n",
            "5141/5141 [==============================] - 2s 305us/step - loss: 0.5154 - accuracy: 0.7395\n",
            "Epoch 49/50\n",
            "5141/5141 [==============================] - 2s 296us/step - loss: 0.5141 - accuracy: 0.7460\n",
            "Epoch 50/50\n",
            "5141/5141 [==============================] - 2s 294us/step - loss: 0.5112 - accuracy: 0.7485\n",
            "5139/5139 [==============================] - 1s 131us/step\n",
            "Training model: model_6, Fold: 2\n",
            "Epoch 1/50\n",
            "10280/10280 [==============================] - 4s 431us/step - loss: 0.6623 - accuracy: 0.6134\n",
            "Epoch 2/50\n",
            "10280/10280 [==============================] - 4s 418us/step - loss: 0.6288 - accuracy: 0.6650\n",
            "Epoch 3/50\n",
            "10280/10280 [==============================] - 4s 419us/step - loss: 0.6116 - accuracy: 0.6839\n",
            "Epoch 4/50\n",
            "10280/10280 [==============================] - 4s 422us/step - loss: 0.6059 - accuracy: 0.6839\n",
            "Epoch 5/50\n",
            "10280/10280 [==============================] - 4s 423us/step - loss: 0.5992 - accuracy: 0.6871\n",
            "Epoch 6/50\n",
            "10280/10280 [==============================] - 4s 425us/step - loss: 0.5945 - accuracy: 0.6883\n",
            "Epoch 7/50\n",
            "10280/10280 [==============================] - 4s 420us/step - loss: 0.5925 - accuracy: 0.6897\n",
            "Epoch 8/50\n",
            "10280/10280 [==============================] - 4s 423us/step - loss: 0.5878 - accuracy: 0.6921\n",
            "Epoch 9/50\n",
            "10280/10280 [==============================] - 4s 424us/step - loss: 0.5871 - accuracy: 0.6927\n",
            "Epoch 10/50\n",
            "10280/10280 [==============================] - 4s 429us/step - loss: 0.5822 - accuracy: 0.6934\n",
            "Epoch 11/50\n",
            "10280/10280 [==============================] - 4s 427us/step - loss: 0.5790 - accuracy: 0.6964\n",
            "Epoch 12/50\n",
            "10280/10280 [==============================] - 4s 426us/step - loss: 0.5754 - accuracy: 0.6982\n",
            "Epoch 13/50\n",
            "10280/10280 [==============================] - 4s 429us/step - loss: 0.5762 - accuracy: 0.6961\n",
            "Epoch 14/50\n",
            "10280/10280 [==============================] - 4s 427us/step - loss: 0.5689 - accuracy: 0.6994\n",
            "Epoch 15/50\n",
            "10280/10280 [==============================] - 4s 421us/step - loss: 0.5673 - accuracy: 0.7004\n",
            "Epoch 16/50\n",
            "10280/10280 [==============================] - 4s 418us/step - loss: 0.5669 - accuracy: 0.6986\n",
            "Epoch 17/50\n",
            "10280/10280 [==============================] - 4s 418us/step - loss: 0.5624 - accuracy: 0.7032\n",
            "Epoch 18/50\n",
            "10280/10280 [==============================] - 4s 421us/step - loss: 0.5594 - accuracy: 0.7055\n",
            "Epoch 19/50\n",
            "10280/10280 [==============================] - 4s 418us/step - loss: 0.5586 - accuracy: 0.7080\n",
            "Epoch 20/50\n",
            "10280/10280 [==============================] - 4s 423us/step - loss: 0.5573 - accuracy: 0.7079\n",
            "Epoch 21/50\n",
            "10280/10280 [==============================] - 4s 425us/step - loss: 0.5547 - accuracy: 0.7120\n",
            "Epoch 22/50\n",
            "10280/10280 [==============================] - 4s 428us/step - loss: 0.5530 - accuracy: 0.7093\n",
            "Epoch 23/50\n",
            "10280/10280 [==============================] - 4s 427us/step - loss: 0.5529 - accuracy: 0.7130\n",
            "Epoch 24/50\n",
            "10280/10280 [==============================] - 4s 426us/step - loss: 0.5482 - accuracy: 0.7170\n",
            "Epoch 25/50\n",
            "10280/10280 [==============================] - 4s 418us/step - loss: 0.5481 - accuracy: 0.7147\n",
            "Epoch 26/50\n",
            "10280/10280 [==============================] - 4s 420us/step - loss: 0.5462 - accuracy: 0.7171\n",
            "Epoch 27/50\n",
            "10280/10280 [==============================] - 4s 425us/step - loss: 0.5445 - accuracy: 0.7172\n",
            "Epoch 28/50\n",
            "10280/10280 [==============================] - 4s 425us/step - loss: 0.5413 - accuracy: 0.7189\n",
            "Epoch 29/50\n",
            "10280/10280 [==============================] - 4s 424us/step - loss: 0.5392 - accuracy: 0.7230\n",
            "Epoch 30/50\n",
            "10280/10280 [==============================] - 4s 418us/step - loss: 0.5368 - accuracy: 0.7226\n",
            "Epoch 31/50\n",
            "10280/10280 [==============================] - 4s 423us/step - loss: 0.5342 - accuracy: 0.7237\n",
            "Epoch 32/50\n",
            "10280/10280 [==============================] - 4s 430us/step - loss: 0.5337 - accuracy: 0.7275\n",
            "Epoch 33/50\n",
            "10280/10280 [==============================] - 4s 429us/step - loss: 0.5336 - accuracy: 0.7324\n",
            "Epoch 34/50\n",
            "10280/10280 [==============================] - 4s 426us/step - loss: 0.5294 - accuracy: 0.7277\n",
            "Epoch 35/50\n",
            "10280/10280 [==============================] - 4s 432us/step - loss: 0.5292 - accuracy: 0.7328\n",
            "Epoch 36/50\n",
            "10280/10280 [==============================] - 4s 428us/step - loss: 0.5286 - accuracy: 0.7315\n",
            "Epoch 37/50\n",
            "10280/10280 [==============================] - 4s 416us/step - loss: 0.5261 - accuracy: 0.7335\n",
            "Epoch 38/50\n",
            "10280/10280 [==============================] - 4s 423us/step - loss: 0.5229 - accuracy: 0.7368\n",
            "Epoch 39/50\n",
            "10280/10280 [==============================] - 4s 425us/step - loss: 0.5220 - accuracy: 0.7393\n",
            "Epoch 40/50\n",
            "10280/10280 [==============================] - 4s 424us/step - loss: 0.5210 - accuracy: 0.7364\n",
            "Epoch 41/50\n",
            "10280/10280 [==============================] - 4s 434us/step - loss: 0.5205 - accuracy: 0.7389\n",
            "Epoch 42/50\n",
            "10280/10280 [==============================] - 4s 428us/step - loss: 0.5186 - accuracy: 0.7416\n",
            "Epoch 43/50\n",
            "10280/10280 [==============================] - 4s 433us/step - loss: 0.5168 - accuracy: 0.7397\n",
            "Epoch 44/50\n",
            "10280/10280 [==============================] - 4s 418us/step - loss: 0.5149 - accuracy: 0.7395\n",
            "Epoch 45/50\n",
            "10280/10280 [==============================] - 4s 423us/step - loss: 0.5118 - accuracy: 0.7449\n",
            "Epoch 46/50\n",
            "10280/10280 [==============================] - 4s 422us/step - loss: 0.5102 - accuracy: 0.7455\n",
            "Epoch 47/50\n",
            "10280/10280 [==============================] - 4s 425us/step - loss: 0.5117 - accuracy: 0.7446\n",
            "Epoch 48/50\n",
            "10280/10280 [==============================] - 4s 420us/step - loss: 0.5097 - accuracy: 0.7469\n",
            "Epoch 49/50\n",
            "10280/10280 [==============================] - 4s 424us/step - loss: 0.5091 - accuracy: 0.7450\n",
            "Epoch 50/50\n",
            "10280/10280 [==============================] - 4s 432us/step - loss: 0.5078 - accuracy: 0.7466\n",
            "5139/5139 [==============================] - 1s 143us/step\n",
            "Training model: model_6, Fold: 3\n",
            "Epoch 1/50\n",
            "15419/15419 [==============================] - 8s 501us/step - loss: 0.6631 - accuracy: 0.6177\n",
            "Epoch 2/50\n",
            "15419/15419 [==============================] - 8s 492us/step - loss: 0.6253 - accuracy: 0.6731\n",
            "Epoch 3/50\n",
            "15419/15419 [==============================] - 8s 491us/step - loss: 0.6125 - accuracy: 0.6828\n",
            "Epoch 4/50\n",
            "15419/15419 [==============================] - 8s 487us/step - loss: 0.6065 - accuracy: 0.6838\n",
            "Epoch 5/50\n",
            "15419/15419 [==============================] - 7s 486us/step - loss: 0.6006 - accuracy: 0.6851\n",
            "Epoch 6/50\n",
            "15419/15419 [==============================] - 8s 488us/step - loss: 0.5973 - accuracy: 0.6858\n",
            "Epoch 7/50\n",
            "15419/15419 [==============================] - 8s 487us/step - loss: 0.5939 - accuracy: 0.6867\n",
            "Epoch 8/50\n",
            "15419/15419 [==============================] - 8s 491us/step - loss: 0.5924 - accuracy: 0.6880\n",
            "Epoch 9/50\n",
            "15419/15419 [==============================] - 8s 490us/step - loss: 0.5888 - accuracy: 0.6895\n",
            "Epoch 10/50\n",
            "15419/15419 [==============================] - 7s 485us/step - loss: 0.5854 - accuracy: 0.6903\n",
            "Epoch 11/50\n",
            "15419/15419 [==============================] - 8s 488us/step - loss: 0.5822 - accuracy: 0.6912\n",
            "Epoch 12/50\n",
            "15419/15419 [==============================] - 7s 481us/step - loss: 0.5782 - accuracy: 0.6949\n",
            "Epoch 13/50\n",
            "15419/15419 [==============================] - 7s 481us/step - loss: 0.5773 - accuracy: 0.6941\n",
            "Epoch 14/50\n",
            "15419/15419 [==============================] - 7s 485us/step - loss: 0.5739 - accuracy: 0.6971\n",
            "Epoch 15/50\n",
            "15419/15419 [==============================] - 7s 482us/step - loss: 0.5739 - accuracy: 0.6954\n",
            "Epoch 16/50\n",
            "15419/15419 [==============================] - 8s 488us/step - loss: 0.5693 - accuracy: 0.7008\n",
            "Epoch 17/50\n",
            "15419/15419 [==============================] - 7s 484us/step - loss: 0.5683 - accuracy: 0.7005\n",
            "Epoch 18/50\n",
            "15419/15419 [==============================] - 7s 484us/step - loss: 0.5647 - accuracy: 0.7026\n",
            "Epoch 19/50\n",
            "15419/15419 [==============================] - 7s 485us/step - loss: 0.5633 - accuracy: 0.7058\n",
            "Epoch 20/50\n",
            "15419/15419 [==============================] - 7s 480us/step - loss: 0.5614 - accuracy: 0.7043\n",
            "Epoch 21/50\n",
            "15419/15419 [==============================] - 7s 483us/step - loss: 0.5595 - accuracy: 0.7063\n",
            "Epoch 22/50\n",
            "15419/15419 [==============================] - 7s 485us/step - loss: 0.5599 - accuracy: 0.7066\n",
            "Epoch 23/50\n",
            "15419/15419 [==============================] - 7s 477us/step - loss: 0.5554 - accuracy: 0.7089\n",
            "Epoch 24/50\n",
            "15419/15419 [==============================] - 7s 481us/step - loss: 0.5533 - accuracy: 0.7106\n",
            "Epoch 25/50\n",
            "15419/15419 [==============================] - 7s 474us/step - loss: 0.5545 - accuracy: 0.7099\n",
            "Epoch 26/50\n",
            "15419/15419 [==============================] - 7s 482us/step - loss: 0.5508 - accuracy: 0.7139\n",
            "Epoch 27/50\n",
            "15419/15419 [==============================] - 7s 474us/step - loss: 0.5498 - accuracy: 0.7122\n",
            "Epoch 28/50\n",
            "15419/15419 [==============================] - 7s 482us/step - loss: 0.5490 - accuracy: 0.7159\n",
            "Epoch 29/50\n",
            "15419/15419 [==============================] - 7s 482us/step - loss: 0.5452 - accuracy: 0.7163\n",
            "Epoch 30/50\n",
            "15419/15419 [==============================] - 7s 478us/step - loss: 0.5454 - accuracy: 0.7189\n",
            "Epoch 31/50\n",
            "15419/15419 [==============================] - 7s 480us/step - loss: 0.5436 - accuracy: 0.7178\n",
            "Epoch 32/50\n",
            "15419/15419 [==============================] - 7s 483us/step - loss: 0.5424 - accuracy: 0.7220\n",
            "Epoch 33/50\n",
            "15419/15419 [==============================] - 7s 482us/step - loss: 0.5400 - accuracy: 0.7218\n",
            "Epoch 34/50\n",
            "15419/15419 [==============================] - 7s 479us/step - loss: 0.5389 - accuracy: 0.7251\n",
            "Epoch 35/50\n",
            "15419/15419 [==============================] - 7s 480us/step - loss: 0.5357 - accuracy: 0.7254\n",
            "Epoch 36/50\n",
            "15419/15419 [==============================] - 7s 482us/step - loss: 0.5334 - accuracy: 0.7284\n",
            "Epoch 37/50\n",
            "15419/15419 [==============================] - 8s 487us/step - loss: 0.5343 - accuracy: 0.7286\n",
            "Epoch 38/50\n",
            "15419/15419 [==============================] - 7s 480us/step - loss: 0.5315 - accuracy: 0.7286\n",
            "Epoch 39/50\n",
            "15419/15419 [==============================] - 7s 480us/step - loss: 0.5307 - accuracy: 0.7305\n",
            "Epoch 40/50\n",
            "15419/15419 [==============================] - 7s 479us/step - loss: 0.5301 - accuracy: 0.7343\n",
            "Epoch 41/50\n",
            "15419/15419 [==============================] - 7s 479us/step - loss: 0.5279 - accuracy: 0.7346\n",
            "Epoch 42/50\n",
            "15419/15419 [==============================] - 7s 477us/step - loss: 0.5290 - accuracy: 0.7357\n",
            "Epoch 43/50\n",
            "15419/15419 [==============================] - 7s 480us/step - loss: 0.5241 - accuracy: 0.7312\n",
            "Epoch 44/50\n",
            "15419/15419 [==============================] - 7s 479us/step - loss: 0.5261 - accuracy: 0.7331\n",
            "Epoch 45/50\n",
            "15419/15419 [==============================] - 7s 477us/step - loss: 0.5222 - accuracy: 0.7358\n",
            "Epoch 46/50\n",
            "15419/15419 [==============================] - 7s 479us/step - loss: 0.5225 - accuracy: 0.7384\n",
            "Epoch 47/50\n",
            "15419/15419 [==============================] - 7s 476us/step - loss: 0.5224 - accuracy: 0.7353\n",
            "Epoch 48/50\n",
            "15419/15419 [==============================] - 7s 478us/step - loss: 0.5204 - accuracy: 0.7379\n",
            "Epoch 49/50\n",
            "15419/15419 [==============================] - 7s 476us/step - loss: 0.5209 - accuracy: 0.7401\n",
            "Epoch 50/50\n",
            "15419/15419 [==============================] - 7s 476us/step - loss: 0.5199 - accuracy: 0.7384\n",
            "5139/5139 [==============================] - 1s 142us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "196g9iCbqcVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "cfbb3f3d-3be4-41ee-d3d1-74c3171ec16b"
      },
      "source": [
        "run1_metrics = loss_and_metrics\n",
        "run1_metrics"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_1': {1: [0.7175507575163328, 0.5962249636650085],\n",
              "  2: [0.7793352637547977, 0.5539988279342651],\n",
              "  3: [0.7826963207396489, 0.5483556985855103]},\n",
              " 'model_2': {1: [0.6633978218251941, 0.6063436269760132],\n",
              "  2: [0.6620032916298217, 0.5866900086402893],\n",
              "  3: [0.6733418847248466, 0.5670363903045654]},\n",
              " 'model_3': {1: [0.6824695722535788, 0.6322241425514221],\n",
              "  2: [0.7345550649243585, 0.5139132142066956],\n",
              "  3: [0.7718580756746022, 0.4928974509239197]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YELtvJhd5yX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "990d5db0-b236-47b9-a456-eab3fe45ca9f"
      },
      "source": [
        "loss_and_metrics"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_6': {1: [0.6284209717268794, 0.6763961911201477],\n",
              "  2: [0.6330148733457858, 0.655964195728302],\n",
              "  3: [0.6457746217259639, 0.6598560214042664]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-ixfhtIrGCC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f2f47fd5-8182-4d5f-eb8c-40f60f77b0ad"
      },
      "source": [
        "def print_average_metrics(loss_and_metrics):\n",
        "    for model_name, model_folds in loss_and_metrics.items():\n",
        "        sum_list = []\n",
        "        for i, fold in model_folds.items():\n",
        "            sum_list.append(fold[1])\n",
        "            'folds:'\n",
        "        print(f'average for model {model_name}')\n",
        "        print(sum(sum_list)/len(model_folds))\n",
        "\n",
        "print_average_metrics(loss_and_metrics)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average for model model_6\n",
            "0.6640721360842387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hUdO5rBiktrw",
        "outputId": "4145fb67-5ea7-4532-ed6a-623578bddf6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "!ls -lsa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 248\n",
            "  0 drwxr-xr-x  7 Misha  staff    224 Mar 28 21:21 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
            "  0 drwxr-xr-x  9 Misha  staff    288 Mar 28 17:35 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
            "  0 drwxr-xr-x  6 Misha  staff    192 Mar 26 23:41 \u001b[1m\u001b[36m.ipynb_checkpoints\u001b[m\u001b[m\n",
            " 32 -rw-r--r--  1 Misha  staff  15084 Mar 28 14:22 decision_tree.ipynb\n",
            "144 -rw-r--r--  1 Misha  staff  70555 Mar 26 23:41 knn_classifier.ipynb\n",
            " 24 -rw-r--r--  1 Misha  staff   9549 Mar 28 20:08 knn_v2.ipynb\n",
            " 48 -rw-r--r--  1 Misha  staff  23091 Mar 28 21:21 nn.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FXdEfk8U8HXz",
        "colab": {}
      },
      "source": [
        "# Final test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JbFZhQCqcVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mtts = MultipleTrainTestSplits(csv_path=csv_path)\n",
        "pp = Preprocessor(path_to_raw_data=csv_path)\n",
        "\n",
        "train_validation_set = mtts.train_validation_set\n",
        "test_set = mtts.test_set\n",
        "\n",
        "# Preprocess the training+validation\n",
        "preprocessed_train_validation_set = pp.preprocess(train_validation_set)\n",
        "# Split the features from the target\n",
        "x_train_val = get_x(preprocessed_train_validation_set)\n",
        "y_train_val = get_y(preprocessed_train_validation_set)\n",
        "\n",
        "# Preprocess the test set\n",
        "preprocessed_test_set = pp.preprocess(test_set)\n",
        "# Split the features from the target\n",
        "x_test = get_x(preprocessed_test_set)\n",
        "y_test = get_y(preprocessed_test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzTBxQfRqcVs",
        "colab_type": "code",
        "colab": {},
        "outputId": "9efb0190-c7c5-4d5d-bdef-e5bb22cca49f"
      },
      "source": [
        "input_dim = x_train_val.shape[1]  # number of columns (dimensions for the input layer of the model)\n",
        "\n",
        "# Winning model here\n",
        "model = create_model_1(input_dim)\n",
        "\n",
        "model.fit(x_train_val, y_train_val, epochs=1, batch_size=128)\n",
        "\n",
        "final_loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "20558/20558 [==============================] - 1s 59us/step - loss: 3.7973 - accuracy: 0.5687\n",
            "5139/5139 [==============================] - 0s 34us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFvPd_EnqcVu",
        "colab_type": "code",
        "colab": {},
        "outputId": "13293a6d-8011-442e-b655-b1a1e977e438"
      },
      "source": [
        "print(final_loss_and_metrics)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7411688035576927, 0.5829927921295166]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}