{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/misharigot/kobe/blob/master/src/model/nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pl5KceKr8HWB"
   },
   "source": [
    "This notebook contains the neural network to predict kobe's shots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQF73eWo8HWF"
   },
   "source": [
    "## To Do from Trello\n",
    "- [x] Implementeren van cross validation.\n",
    "- [ ] Connecten van nieuwe cross validation module met de nn model module.\n",
    "- [ ] Bouwen van verschillende netwerken (vorm, aantal nodes etc.)\n",
    "- [ ] Kijken welke loss function we moeten gebruiken, cross entropy vs log loss. Log loss sowieso proberen om te vergelijken met competition entries.\n",
    "- [ ] Implementeren van model export functie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "UNP7SAMudAib",
    "outputId": "a97318f1-98de-49f5-9c31-1cc4db31e160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'kobe'...\n",
      "remote: Enumerating objects: 178, done.\u001b[K\n",
      "remote: Counting objects: 100% (178/178), done.\u001b[K\n",
      "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
      "remote: Total 178 (delta 74), reused 126 (delta 34), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (178/178), 1.36 MiB | 19.90 MiB/s, done.\n",
      "Resolving deltas: 100% (74/74), done.\n"
     ]
    }
   ],
   "source": [
    "# When using this notebook in Google Colab, clone the repo in the file system in\n",
    "# order to use the python modules from the repo.\n",
    "!git  clone https://github.com/misharigot/kobe.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "P32ha3sG8HWH",
    "outputId": "3754cefe-c89f-4675-8f71-538127099cae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.insert(0, '..')  # Needed to make the import below work\n",
    "\n",
    "# Use the line below in Colab\n",
    "# from kobe.src.multiple_train_test_splits import MultipleTrainTestSplits\n",
    "\n",
    "# Use the line below in a local env\n",
    "from multiple_train_test_splits import MultipleTrainTestSplits\n",
    "from preprocessor import Preprocessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JS4xXHyQGCCv"
   },
   "outputs": [],
   "source": [
    "def get_x(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Returns the features.\n",
    "    \"\"\"\n",
    "    X = data.drop(columns=['shot_made_flag'])\n",
    "    return X\n",
    "\n",
    "def get_y(data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Returns the target.\n",
    "    \"\"\"\n",
    "    Y = data['shot_made_flag'].copy()\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0Rei6a8GCC1"
   },
   "outputs": [],
   "source": [
    "def create_model_1(input_dim: int):\n",
    "    \"\"\"Simple one hidden layer network.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=32, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_2(input_dim: int):\n",
    "    \"\"\"2 hidden layers network.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_3(input_dim: int):\n",
    "    \"\"\"1 hidden layer network with a lot of neurons.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=int(input_dim/2), activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_models_dict():\n",
    "    models = {}\n",
    "    models['model_1'] = create_model_1\n",
    "    models['model_2'] = create_model_2\n",
    "    models['model_3'] = create_model_3\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime as dt\n",
    "# first_recorded_game = str(dt.datetime.strptime(\n",
    "#             min(pp.raw_data['game_date']), '%Y-%m-%d').strftime('%Y-%m-%d'))\n",
    "# print(first_recorded_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use in Colab\n",
    "# csv_path = 'kobe/data/data.csv'\n",
    "\n",
    "# Use in local env\n",
    "csv_path = '../../data/data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "5sbi8PrU8HXm",
    "outputId": "712223af-c93d-4694-ec10-6de46847d035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: model_1, Fold: 1\n",
      "Epoch 1/1\n",
      "5141/5141 [==============================] - 3s 599us/step - loss: 4.5688 - accuracy: 0.5240\n",
      "5139/5139 [==============================] - 2s 312us/step\n",
      "Training model: model_1, Fold: 2\n",
      "Epoch 1/1\n",
      "10280/10280 [==============================] - 7s 633us/step - loss: 4.2571 - accuracy: 0.5422\n",
      "5139/5139 [==============================] - 2s 300us/step\n",
      "Training model: model_1, Fold: 3\n",
      "Epoch 1/1\n",
      "15419/15419 [==============================] - 11s 685us/step - loss: 4.6482 - accuracy: 0.5231\n",
      "5139/5139 [==============================] - 2s 312us/step\n",
      "Training model: model_2, Fold: 1\n",
      "Epoch 1/1\n",
      "5141/5141 [==============================] - 3s 605us/step - loss: 0.7360 - accuracy: 0.5345\n",
      "5139/5139 [==============================] - 2s 309us/step\n",
      "Training model: model_2, Fold: 2\n",
      "Epoch 1/1\n",
      "10280/10280 [==============================] - 7s 642us/step - loss: 0.8133 - accuracy: 0.5305\n",
      "5139/5139 [==============================] - 2s 322us/step\n",
      "Training model: model_2, Fold: 3\n",
      "Epoch 1/1\n",
      "15419/15419 [==============================] - 10s 677us/step - loss: 0.7866 - accuracy: 0.5506\n",
      "5139/5139 [==============================] - 2s 305us/step\n",
      "Training model: model_3, Fold: 1\n",
      "Epoch 1/1\n",
      "5141/5141 [==============================] - 3s 650us/step - loss: 0.8172 - accuracy: 0.5598\n",
      "5139/5139 [==============================] - 2s 321us/step\n",
      "Training model: model_3, Fold: 2\n",
      "Epoch 1/1\n",
      "10280/10280 [==============================] - 7s 711us/step - loss: 0.9570 - accuracy: 0.5553\n",
      "5139/5139 [==============================] - 2s 313us/step\n",
      "Training model: model_3, Fold: 3\n",
      "Epoch 1/1\n",
      "15419/15419 [==============================] - 11s 734us/step - loss: 1.0124 - accuracy: 0.5605\n",
      "5139/5139 [==============================] - 2s 342us/step\n"
     ]
    }
   ],
   "source": [
    "mtts = MultipleTrainTestSplits(csv_path=csv_path)\n",
    "pp = Preprocessor(path_to_raw_data=csv_path)\n",
    "\n",
    "test_set = mtts.test_set\n",
    "\n",
    "loss_and_metrics = {}\n",
    "models = get_models_dict()\n",
    "\n",
    "# Loop over the models\n",
    "for model_name, model_func in models.items():\n",
    "    # checkpoint_path = \"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    \n",
    "    loss_and_metrics[model_name] = {}\n",
    "    \n",
    "    # Loop over the train/validation splits/folds\n",
    "    n_fold = 0\n",
    "    for train_set, validation_set in mtts.train_validation_split(as_dataframe=True):\n",
    "        n_fold += 1\n",
    "        print(f'Training model: {model_name}, Fold: {n_fold}')\n",
    "        checkpoint_path = f\"{model_name}_fold_{n_fold}_weights-improvement\" + \"-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "\n",
    "        # Preprocess the training set\n",
    "        preprocessed_train_set = pp.preprocess(train_set)\n",
    "        # Split the features from the target\n",
    "        x_train = get_x(preprocessed_train_set)\n",
    "        y_train = get_y(preprocessed_train_set)\n",
    "\n",
    "        # Preprocess the validation set\n",
    "        preprocessed_validation_set= pp.preprocess(validation_set)\n",
    "        # Split the features from the target\n",
    "        x_validation = get_x(preprocessed_validation_set)\n",
    "        y_validation = get_y(preprocessed_validation_set)\n",
    "\n",
    "        input_dim = x_train.shape[1]  # number of columns (dimensions for the input layer of the model)\n",
    "        \n",
    "        # model = create_model(input_dim=input_dim)\n",
    "        model = model_func(input_dim)\n",
    "\n",
    "#         # Create model checkpoint to be able to resume at a checkpoint when training crashes.\n",
    "#         checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#         callbacks_list = [checkpoint]\n",
    "\n",
    "        # Fit the model\n",
    "#         model.fit(x_train, y_train, epochs=2, batch_size=10, \n",
    "#                   validation_data=(x_validation, y_validation),\n",
    "#                   callbacks=callbacks_list, verbose=0)\n",
    "        model.fit(x_train, y_train, epochs=1, batch_size=128)\n",
    "    \n",
    "        loss_and_metrics[model_name][n_fold] = model.evaluate(x_validation, y_validation, batch_size=128)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_1': {1: [0.8631778114337034, 0.5641175508499146],\n",
       "  2: [0.8561778308979197, 0.540182888507843],\n",
       "  3: [1.7360578366443093, 0.5395991206169128]},\n",
       " 'model_2': {1: [0.6847195672163079, 0.5765713453292847],\n",
       "  2: [0.6894150146437795, 0.540182888507843],\n",
       "  3: [0.6744304086290275, 0.5866900086402893]},\n",
       " 'model_3': {1: [1.1515919211588612, 0.5641175508499146],\n",
       "  2: [0.6852728027526357, 0.6337808966636658],\n",
       "  3: [1.7372203638266441, 0.46040084958076477]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ixfhtIrGCC8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_and_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-88ae70ed9991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint_average_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_and_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_and_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "def print_average_metrics(loss_and_metrics):\n",
    "    for model_name, model_folds in loss_and_metrics.items():\n",
    "        sum_list = []\n",
    "        print('folds')\n",
    "        for i, fold in model_folds.items():\n",
    "            sum_list.append(fold[1])\n",
    "            'folds:'\n",
    "        print(f'average for model {model_name}')\n",
    "        print(sum(sum_list)/len(model_folds))\n",
    "\n",
    "print_average_metrics(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "hUdO5rBiktrw",
    "outputId": "4145fb67-5ea7-4532-ed6a-623578bddf6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 248\n",
      "  0 drwxr-xr-x  7 Misha  staff    224 Mar 28 21:21 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "  0 drwxr-xr-x  9 Misha  staff    288 Mar 28 17:35 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "  0 drwxr-xr-x  6 Misha  staff    192 Mar 26 23:41 \u001b[1m\u001b[36m.ipynb_checkpoints\u001b[m\u001b[m\n",
      " 32 -rw-r--r--  1 Misha  staff  15084 Mar 28 14:22 decision_tree.ipynb\n",
      "144 -rw-r--r--  1 Misha  staff  70555 Mar 26 23:41 knn_classifier.ipynb\n",
      " 24 -rw-r--r--  1 Misha  staff   9549 Mar 28 20:08 knn_v2.ipynb\n",
      " 48 -rw-r--r--  1 Misha  staff  23091 Mar 28 21:21 nn.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls -lsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXdEfk8U8HXz",
    "outputId": "93c02ec8-0302-466c-98da-328924921432"
   },
   "source": [
    "# Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtts = MultipleTrainTestSplits(csv_path=csv_path)\n",
    "pp = Preprocessor(path_to_raw_data=csv_path)\n",
    "\n",
    "train_validation_set = mtts.train_validation_set\n",
    "test_set = mtts.test_set\n",
    "\n",
    "# Preprocess the training+validation\n",
    "preprocessed_train_validation_set = pp.preprocess(train_validation_set)\n",
    "# Split the features from the target\n",
    "x_train_val = get_x(preprocessed_train_validation_set)\n",
    "y_train_val = get_y(preprocessed_train_validation_set)\n",
    "\n",
    "# Preprocess the test set\n",
    "preprocessed_test_set = pp.preprocess(test_set)\n",
    "# Split the features from the target\n",
    "x_test = get_x(preprocessed_test_set)\n",
    "y_test = get_y(preprocessed_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "20558/20558 [==============================] - 1s 59us/step - loss: 3.7973 - accuracy: 0.5687\n",
      "5139/5139 [==============================] - 0s 34us/step\n"
     ]
    }
   ],
   "source": [
    "input_dim = x_train_val.shape[1]  # number of columns (dimensions for the input layer of the model)\n",
    "\n",
    "# Winning model here\n",
    "model = create_model_1(input_dim)\n",
    "\n",
    "model.fit(x_train_val, y_train_val, epochs=1, batch_size=128)\n",
    "\n",
    "final_loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7411688035576927, 0.5829927921295166]\n"
     ]
    }
   ],
   "source": [
    "print(final_loss_and_metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kobe",
   "language": "python",
   "name": "kobe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
