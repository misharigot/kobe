{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/misharigot/kobe/blob/master/src/model/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pl5KceKr8HWB"
   },
   "source": [
    "This notebook contains the neural network to predict kobe's shots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQF73eWo8HWF"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P32ha3sG8HWH",
    "outputId": "57c6a5d5-0c5d-4301-d86c-be20ad40023d",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')  # Needed to make the import below work\n",
    "\n",
    "from multiple_train_test_splits import MultipleTrainTestSplits\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as ft\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def combine_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Combine the minutes and seconds remaining columns into one column.\n",
    "    \"\"\"\n",
    "    df['minutes_remaining'] = df['minutes_remaining'].astype(int)\n",
    "    df['seconds_remaining'] = df['seconds_remaining'].astype(int)\n",
    "\n",
    "    # Combine minutes and seconds remaining into decimal minutes remaining, e.g. 6.5 for 6 mins and 30 secs.\n",
    "    df['time_remaining'] = round(df['minutes_remaining'] + (df['seconds_remaining'] / 60), 2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df: pd.DataFrame, encoder: preprocessing.OneHotEncoder = None) -> pd.DataFrame:\n",
    "    \"\"\"One-hot encode all categorical columns.\n",
    "    Optionally provide an encoder. Use the training set encoder to one-hot encode the test set.\n",
    "    \"\"\"\n",
    "     # Categorize all columns based on their data type\n",
    "    categorical_columns = [\n",
    "        'action_type',\n",
    "        'combined_shot_type',\n",
    "        'game_event_id', # Meaning?\n",
    "        'game_id',\n",
    "        'season',\n",
    "        'shot_type',\n",
    "        'shot_zone_area',\n",
    "        'shot_zone_basic',\n",
    "        'shot_zone_range',\n",
    "        'team_id',\n",
    "        'team_name',\n",
    "        'matchup',\n",
    "        'opponent'\n",
    "    ]\n",
    "\n",
    "    temporal_columns = [\n",
    "        'game_date'\n",
    "    ]\n",
    "\n",
    "    remaining_columns = [\n",
    "        'lat',\n",
    "        'loc_x',\n",
    "        'loc_y',\n",
    "        'lon',\n",
    "        'period',\n",
    "        'shot_distance',\n",
    "        'time_remaining',\n",
    "        'shot_made_flag'  # y label\n",
    "    ]\n",
    "\n",
    "    excluded_columns = [\n",
    "        'shot_id',            # Just an auto-increment id, does not mean anything\n",
    "        'minutes_remaining',  # Not needed, since we use the engineered field 'time_remaining'\n",
    "        'seconds_remaining'   # Not needed, since we use the engineered field 'time_remaining'\n",
    "    ]\n",
    "\n",
    "    # Convert relevant columns to categorical columns\n",
    "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
    "    df_with_only_categoricals = df[categorical_columns]\n",
    "\n",
    "    # One hot encode categorical columns\n",
    "    if encoder is None:\n",
    "        encoder = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "        encoder.fit(df_with_only_categoricals)\n",
    "    one_hot_encoded_df = pd.DataFrame(encoder.transform(df_with_only_categoricals).toarray())\n",
    "    \n",
    "\n",
    "    # Combine the one hot encoded part of the df with the remaining df\n",
    "    non_categorical_df = df[remaining_columns]\n",
    "    resulting_df = pd.concat([one_hot_encoded_df, non_categorical_df], axis=1)\n",
    "    return resulting_df, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_x(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Returns the features.\n",
    "    \"\"\"\n",
    "    X = data.drop(columns=['shot_made_flag'])\n",
    "    return X\n",
    "\n",
    "def get_y(data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Returns the target.\n",
    "    \"\"\"\n",
    "    Y = data['shot_made_flag'].copy()\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# # Plot the decision boundary with the first 50 points in the test set\n",
    "# numpy_x = train[['stature','span']].as_matrix()\n",
    "# numpy_y = train.Gender.cat.codes.values\n",
    "\n",
    "# # This is necessary if pandas read the CSV files as integers\n",
    "# # (seems to depend on version/OS)\n",
    "# numpy_x = numpy_x.astype(float)\n",
    "\n",
    "# # Rebuild the classifier \n",
    "# # (a classifier trained on pandas data doesn't interoperate well with pure numpy data)\n",
    "# tree = DecisionTreeClassifier()\n",
    "# tree.fit(numpy_x, numpy_y)\n",
    "\n",
    "# plot_decision_regions(numpy_x[:25, :], numpy_y[:25], clf=tree, res=0.1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(data: pd.DataFrame, encoder:preprocessing.OneHotEncoder = None) -> np.array:\n",
    "    \"\"\"Preprocess the raw kobe data from Kaggle.\n",
    "    Optionally provide an encoder. Use the training set encoder to one-hot encode the test set.\n",
    "    \"\"\"\n",
    "    df = combine_time(data)\n",
    "    df, encoder = one_hot_encode(df, encoder)\n",
    "    \n",
    "    return df, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# def print_df():\n",
    "    \n",
    "#     df.info(verbose=True) \n",
    "    \n",
    "# print_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sbi8PrU8HXm",
    "outputId": "79b3a233-10c7-4e46-fe67-a71c3d7917b1",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i: 1 - acc: 0.6199649737302977\n",
      " i: 2 - acc: 0.62405137186223\n",
      " i: 3 - acc: 0.616656937147305\n",
      "\n",
      "Tree depth is:  184\n",
      "\n",
      "Params of the tree are:  {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': 'deprecated', 'random_state': 5, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "mtts = MultipleTrainTestSplits(csv_path='../../data/data.csv')\n",
    "\n",
    "test_set = mtts.test_set\n",
    "\n",
    "loss_and_metrics = []\n",
    "scores = []\n",
    "i = 0\n",
    "\n",
    "for train_set, validation_set in mtts.train_validation_split(as_dataframe=True):\n",
    "    i += 1\n",
    "    # Preprocess the training set\n",
    "    preprocessed_train_set, one_hot_encoder = preprocess(train_set)\n",
    "    # Split the features from the target\n",
    "    x_train = get_x(preprocessed_train_set)\n",
    "    y_train = get_y(preprocessed_train_set)\n",
    "\n",
    "    # Preprocess the validation set (use the one hot encoder that was fit on the training set)\n",
    "    preprocessed_validation_set, _ = preprocess(validation_set, encoder=one_hot_encoder)\n",
    "    # Split the features from the target\n",
    "    x_validation = get_x(preprocessed_validation_set)\n",
    "    y_validation = get_y(preprocessed_validation_set)\n",
    "\n",
    "\n",
    "    tree = DecisionTreeClassifier(random_state = 5)\n",
    "\n",
    "    tree.fit(x_train, y_train.astype('int'))\n",
    "\n",
    "    y_predicted = tree.predict(x_validation)\n",
    "    acc = accuracy_score(y_validation.astype('int'), y_predicted.astype('int'))\n",
    "\n",
    "    print(f' i: {i} - acc: {acc}')\n",
    "\n",
    "depth = tree.get_depth()\n",
    "params = tree.get_params(deep=True)\n",
    "\n",
    "print()\n",
    "print('Tree depth is: ', depth)\n",
    "print()\n",
    "print('Params of the tree are: ', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXdEfk8U8HXz",
    "outputId": "93c02ec8-0302-466c-98da-328924921432",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "classes = model.predict(x_validation, batch_size=128)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
